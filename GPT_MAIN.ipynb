{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e553f9c496484621ae3d32264c8b1420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4d0abd8d5f40cc87f344ad3829631c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f51fa8a4e8441ba3a9e36f08880028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a26fa3653284ce281c4b49ea36e3951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9356485de87846a0a62322a64354ff7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a3f7af72324f0db407930d223fd374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62038ea9e89b48e7a06fea23929126c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff980c66665a4ba78f95c8e1fd81d4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79a7588eb8e4cd9beafce8369f1ad6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a2fbd637b24a69a3ba05c3e447acbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset length: 1000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset and select only the first 1000 entries\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:1000]\")\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Add special tokens and resize tokenizer vocabulary\n",
    "special_tokens_dict = {'additional_special_tokens': ['[NL]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Retrieve and print token IDs to verify\n",
    "nl_token_id = tokenizer.convert_tokens_to_ids('[NL]')\n",
    "pad_token_id = tokenizer.pad_token_id  # Already defined in BERT tokenizer as [PAD]\n",
    "\n",
    "# Verify dataset length to ensure it's limited to 1000\n",
    "print(f\"Loaded dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "\n",
    "for entry in dataset:\n",
    "    text = entry['text']\n",
    "    # Tokenize without truncation to get the full length\n",
    "    encodings = tokenizer(text, truncation=False, return_tensors=\"pt\")\n",
    "    token_count = encodings['input_ids'].shape[1]  # Number of tokens in this sequence\n",
    "    # Append only if token count is within the 511 token limit\n",
    "    if token_count <= 511:\n",
    "        filtered_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom Dataset class with preprocessing\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=10):  # Set max_length to 10\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "\n",
    "        # Preprocess: Replace newlines with the special [NL] token and tokenize\n",
    "        text = text.replace(\"\\n\\n\", \" [NL] \")\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",  # Enable padding to max_length\n",
    "            max_length=self.max_length,  # Max length is now 10\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Get input ids and attention mask from tokenizer\n",
    "        input_ids = encodings['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = encodings['attention_mask'].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Create a triangular attention mask\n",
    "        tri_mask = torch.tril(torch.ones(self.max_length, self.max_length))  # Lower triangular matrix (1's below diagonal)\n",
    "\n",
    "        # Keep the padding tokens as-is in the attention mask\n",
    "        attention_mask = attention_mask * tri_mask\n",
    "\n",
    "        target_ids = input_ids.clone()\n",
    "        # Shift target_ids by 1 position for autoregressive modeling\n",
    "        target_ids = target_ids[1:]\n",
    "        target_ids = torch.cat((target_ids, torch.tensor([pad_token_id])))  # Add pad token ID to the end\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'target_ids': target_ids\n",
    "        }\n",
    "\n",
    "# Instantiate the dataset and dataloader for custom text\n",
    "text_dataset = TextDataset(filtered_data, tokenizer, max_length=512)\n",
    "data_loader = DataLoader(text_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids('[NL]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Positional Encoding Class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=513):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Single attention head class\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, head_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Apply official scaling using sqrt(d_k)\n",
    "        d_k = k.size(-1)\n",
    "        wei = (q @ k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, :T, :T]\n",
    "            wei = wei.masked_fill(mask == 0, float('-1e30'))\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Multi-head self-attention class\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = d_model // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(d_model, head_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Single decoder block class\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Optional: Feed-forward network (FFN)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention and residual connection\n",
    "        attn_out = self.self_attention(x, mask)\n",
    "        x = self.layer_norm1(x + self.dropout1(attn_out))\n",
    "\n",
    "        # Feed-forward network and residual connection\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + self.dropout2(ff_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads=4, num_layers=6, max_len=512, dropout=0.1):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Embed the input tokens and apply positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pass through each decoder block with the mask\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Apply final layer normalization and linear projection\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the decoder-only model with multiple layers\n",
    "vocab_size = tokenizer.vocab_size + len(tokenizer.added_tokens_encoder)\n",
    "d_model = 24\n",
    "num_heads = 4\n",
    "num_layers = 3 # Number of decoder layers\n",
    "max_len = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = DecoderOnlyTransformer(vocab_size=vocab_size, d_model=d_model, num_heads=num_heads, num_layers=num_layers, max_len=max_len, dropout=dropout)\n",
    "data_loader = DataLoader(text_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Pass the input through the model\n",
    "'''for batch in dataloader:\n",
    "    input_ids = batch['input_ids'][0].unsqueeze(0)\n",
    "    attention_mask = batch['attention_mask'][0].unsqueeze(0)\n",
    "\n",
    "    #print(\"Input Shape:\", input_ids.shape)\n",
    "\n",
    "    input_ids = input_ids.to(torch.int64)\n",
    "    attention_mask = attention_mask.to(torch.float32)  # Ensure mask is float for compatibility\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, mask=attention_mask)\n",
    "\n",
    "    #print(\"Model Output:\")\n",
    "    #print(output)\n",
    "    #print(\"Output Shape:\", output.shape)\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'model' is your PyTorch model\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "\n",
    "print(f\"Total model parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Define the training function with progress tracking\n",
    "def train_model(model, dataloader, pad_token_id, epochs=5, learning_rate=1e-4, device='cpu'):\n",
    "    model.to(device)  # Move model to specified device\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function, ignoring the pad token\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        start_time = time.time()  # Start time for epoch\n",
    "\n",
    "        # Initialize progress bar for the epoch\n",
    "        batch_progress = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        for batch in batch_progress:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['target_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids ,mask = mask)  # Pass input through the model\n",
    "\n",
    "            # Reshape outputs and labels for CrossEntropyLoss\n",
    "            logits = outputs.view(-1, outputs.size(-1))  # Flatten outputs to 2D\n",
    "            target = labels.view(-1)  # Flatten labels to 1D\n",
    "\n",
    "            # Compute loss while ignoring pad tokens\n",
    "            loss = criterion(logits, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            batch_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Calculate average loss per epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_duration = time.time() - start_time  # Time taken for epoch\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Set hyperparameters and training device\n",
    "epochs = 1\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the padding token ID (replace with the actual pad token ID from your tokenizer)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Call the training function\n",
    "train_model(model, data_loader, pad_token_id=pad_token_id, epochs=epochs, learning_rate=learning_rate, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def pad_single_token_list(token_list, max_length=512, device='cpu'):\n",
    "    # Use tokenizer's pad method to add padding up to the specified max_length\n",
    "    padded_tokens = tokenizer1.pad(\n",
    "        {\"input_ids\": token_list},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Extract the padded tensor and move it to the specified device\n",
    "    padded_tensor = padded_tokens[\"input_ids\"].squeeze(0).to(device)  # Squeeze to remove batch dimension\n",
    "    return padded_tensor\n",
    "\n",
    "# Top-k sampling function\n",
    "def top_k_sampling(logits, k=3, device='cpu'):\n",
    "    \"\"\"Apply top-k sampling to logits\"\"\"\n",
    "    logits = logits / 1.0  # Adjust temperature if necessary\n",
    "\n",
    "    # Get the top-k values and their indices, move to device\n",
    "    top_k_values, top_k_indices = torch.topk(logits, k)\n",
    "    top_k_values = top_k_values.to(device)\n",
    "    top_k_indices = top_k_indices.to(device)\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = F.softmax(top_k_values, dim=-1)\n",
    "\n",
    "    # Sample one token from the top-k probabilities\n",
    "    selected_index = top_k_indices[torch.multinomial(probabilities, 1)]\n",
    "\n",
    "    return selected_index.item()\n",
    "\n",
    "\n",
    "def autoregressive_inference_topk(model, initial_output, max_length=512, k=3, pad_token_id=0, device='cpu'):\n",
    "    model.to(device)  # Ensure model is on the device\n",
    "    model.eval()\n",
    "\n",
    "    generated_tokens_for_input = [tokenizer.cls_token_id]\n",
    "    generated_tokens_for_output = []\n",
    "    current_output = initial_output.to(device)  # Modified: Move initial_output to device\n",
    "    for i in range(current_output.size(1)):\n",
    "        if i > 500:\n",
    "            break\n",
    "        logits = current_output[0, i]\n",
    "        current_token = top_k_sampling(logits, k, device=device)\n",
    "\n",
    "        if current_token == tokenizer.sep_token_id:\n",
    "            break\n",
    "        generated_tokens_for_input.append(current_token)\n",
    "        generated_tokens_for_output.append(current_token)\n",
    "        current_input = pad_single_token_list(generated_tokens_for_input, max_length = max_length, device=device)  # Modified: Move padded input to device\n",
    "        current_output = model(current_input.unsqueeze(0))\n",
    "    return generated_tokens_for_output\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming tokenizer and model are already initialized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "initial_input = torch.tensor([[tokenizer.cls_token_id] + [pad_token_id] * 511], device=device)  # Modified: Move initial input to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_output = model(initial_input)  # No change needed, as initial_input is already on the correct device\n",
    "\n",
    "# Generate text\n",
    "print(\"\\n\")\n",
    "for i in range(3):\n",
    "   print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "   print(f\"story {i+1} :\")\n",
    "   generated_sequence = autoregressive_inference_topk(model, initial_output=initial_output, max_length=512, k=50, pad_token_id=pad_token_id, device=device)\n",
    "   generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "   generated_text = generated_text.replace(\" [NL] \", \"\\n\").replace(\"[NL] \", \"\\n\").replace(\" [NL]\", \"\\n\").replace(\"[NL]\", \"\\n\")\n",
    "   print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
