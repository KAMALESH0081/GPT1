{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KAMALESH0081/GPT1/blob/main/Story_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dataset and select only the first 1000 entries\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Add special tokens and resize tokenizer vocabulary\n",
        "special_tokens_dict = {'additional_special_tokens': ['[NL]']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# Retrieve and print token IDs to verify\n",
        "nl_token_id = tokenizer.convert_tokens_to_ids('[NL]')\n",
        "pad_token_id = tokenizer.pad_token_id  # Already defined in BERT tokenizer as [PAD]\n",
        "\n",
        "# Verify dataset length to ensure it's limited to 1000\n",
        "print(f\"Loaded dataset length: {len(dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bMy679JbwDzC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = []\n",
        "\n",
        "for entry in dataset:\n",
        "    text = entry['text']\n",
        "    # Tokenize without truncation to get the full length\n",
        "    encodings = tokenizer(text, truncation=False, return_tensors=\"pt\")\n",
        "    token_count = encodings['input_ids'].shape[1]  # Number of tokens in this sequence\n",
        "    # Append only if token count is within the 511 token limit\n",
        "    if token_count <= 295:\n",
        "        filtered_data.append(entry)\n",
        "len(filtered_data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yw99qzI_wDzC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(filtered_data)\n",
        "df['text'] = df['text'].str.lower()\n",
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "RovyPHalwDzD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tokenized'] = df['text'].apply(\n",
        "    lambda x: tokenizer.encode(x, truncation=True, max_length=300, add_special_tokens=False)\n",
        ")\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "lXSHckaOwDzD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"token_length\"] = df[\"Tokenized\"].apply(len)\n",
        "\n",
        "df_sorted = df.sort_values(by=\"token_length\")\n",
        "\n",
        "# Reset the index for the sorted DataFrame\n",
        "df_sorted.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "68b323buwDzD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_target(tokens):\n",
        "    total_tokens = len(tokens)\n",
        "    # Calculate the 75% index\n",
        "    split_index = int(total_tokens * 0.75)\n",
        "\n",
        "    # Create target by replacing first 75% tokens with pad (token id 0 is [PAD])\n",
        "    target_tokens = [0] * split_index + tokens[split_index:]\n",
        "    return target_tokens\n",
        "\n",
        "def create_target_text(tokens):\n",
        "    total_tokens = len(tokens)\n",
        "    # Calculate the 75% index\n",
        "    split_index = int(total_tokens * 0.75)\n",
        "\n",
        "    # Create target by replacing first 75% tokens with pad (token id 0 is [PAD])\n",
        "    target_tokens = tokens[split_index:]\n",
        "    return tokenizer.decode(target_tokens)\n",
        "\n",
        "# Create the target column\n",
        "df['Target_Text'] = df['Tokenized'].apply(create_target_text)\n",
        "df['Target'] = df['Tokenized'].apply(create_target)\n",
        "\n",
        "# Display the DataFrame with tokenized and target columns\n",
        "print(df.head(-1))"
      ],
      "metadata": {
        "trusted": true,
        "id": "gSH3_zbfwDzE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the distribution of total token length using a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_sorted[\"token_length\"], bins=40, kde=True, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Total Token Length\", fontsize=16)\n",
        "plt.xlabel(\"Token Length\", fontsize=14)\n",
        "plt.ylabel(\"Frequency\", fontsize=14)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Boxplot for a quick summary of the distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=df_sorted[\"token_length\"], color=\"lightcoral\")\n",
        "plt.title(\"Boxplot of Total Token Length\", fontsize=16)\n",
        "plt.xlabel(\"Token Length \", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4xqZRBsGwDzE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example of the maximum padding length\n",
        "max_len = 300\n",
        "cls_token = tokenizer.cls_token_id\n",
        "sep_token = tokenizer.sep_token_id\n",
        "\n",
        "# Function to pad sequences\n",
        "def pad_sequence_source(tokens, max_len, cls_token = tokenizer.cls_token):\n",
        "    # Add the cls token at the beginning\n",
        "    padded_tokens = [cls_token] + tokens\n",
        "\n",
        "    # Pad the sequence if it's shorter than max_len\n",
        "    if len(padded_tokens) < max_len:\n",
        "        padded_tokens.extend([0] * (max_len - len(padded_tokens)))\n",
        "    # Truncate if it's longer than max_len\n",
        "    else:\n",
        "        print(\"some problem in padding source ,token length\", len(padded_tokens), max_len)\n",
        "        padded_tokens = padded_tokens[:max_len]\n",
        "\n",
        "    return padded_tokens\n",
        "\n",
        "def pad_sequence_target(tokens, max_len, sep_token = tokenizer.sep_token):\n",
        "    # Add the cls token at the beginning\n",
        "    padded_tokens = tokens + [sep_token]\n",
        "\n",
        "    # Pad the sequence if it's shorter than max_len\n",
        "    if len(padded_tokens) < max_len:\n",
        "        padded_tokens.extend([0] * (max_len - len(padded_tokens)))\n",
        "    # Truncate if it's longer than max_len\n",
        "    else:\n",
        "        print(\"some problem in padding target ,token length\", len(padded_tokens) , max_len )\n",
        "        padded_tokens = padded_tokens[:max_len]\n",
        "\n",
        "    return padded_tokens\n",
        "\n",
        "# Apply padding and add CLS token to both English and Tamil columns\n",
        "df['Padded_source'] = df['Tokenized'].apply(lambda x: pad_sequence_source(x, max_len, cls_token,))\n",
        "df['Padded_target'] = df['Target'].apply(lambda x: pad_sequence_target(x, max_len, sep_token))\n",
        "\n",
        "# Verify the result\n",
        "#print(df[['Padded_English', 'Padded_Tamil','Padded_Tamil_Target']].head(-10))\n",
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "C5wtJVuVwDzE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['Greedy'] = df['Tokenized'].apply(lambda tokens: [tokenizer.cls_token_id] + tokens[:int(len(tokens)*0.75)] )\n",
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "sfdBKIuzwDzF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, pad_token=0):\n",
        "        self.dataframe = dataframe\n",
        "        self.pad_token = pad_token  # Padding value, typically 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the tokenized sequences for English and Tamil\n",
        "        source =  self.dataframe.iloc[idx][\"text\"]\n",
        "        target_text =  self.dataframe.iloc[idx][\"Target_Text\"]\n",
        "        source_tokens =  torch.tensor(self.dataframe.iloc[idx][\"Padded_source\"],  dtype=torch.long)\n",
        "        target_tokens = torch.tensor(self.dataframe.iloc[idx][\"Padded_target\"],  dtype=torch.long)\n",
        "\n",
        "        def causal_mask(size):\n",
        "              mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "              return mask == 0\n",
        "    # Return the sequence and masks in a dictionary\n",
        "        return {\n",
        "            \"source\": source,\n",
        "            \"target_text\": target_text,\n",
        "            \"source_tokens\": source_tokens.clone(),\n",
        "            \"target_tokens\": target_tokens.clone(),\n",
        "            \"decoder_mask\": (target_tokens != self.pad_token).unsqueeze(0).int() & causal_mask(target_tokens.size(0)).clone(),\n",
        "\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "id": "LYl9CLx7wDzF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset_test(Dataset):\n",
        "    def __init__(self, dataframe, pad_token=0):\n",
        "        self.dataframe = dataframe\n",
        "        self.pad_token = pad_token  # Padding value, typically 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the tokenized sequences for English and Tamil\n",
        "        source =  self.dataframe.iloc[idx][\"text\"]\n",
        "        target_text =  self.dataframe.iloc[idx][\"Target_Text\"]\n",
        "        greedy =  self.dataframe.iloc[idx][\"Greedy\"]\n",
        "        source_tokens =  torch.tensor(self.dataframe.iloc[idx][\"Padded_source\"],  dtype=torch.long)\n",
        "        target_tokens = torch.tensor(self.dataframe.iloc[idx][\"Padded_target\"],  dtype=torch.long)\n",
        "\n",
        "        def causal_mask(size):\n",
        "              mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "              return mask == 0\n",
        "    # Return the sequence and masks in a dictionary\n",
        "        return {\n",
        "            \"source\": source,\n",
        "            \"target_text\": target_text,\n",
        "            \"greedy\": greedy,\n",
        "            \"source_tokens\": source_tokens.clone(),\n",
        "            \"target_tokens\": target_tokens.clone(),\n",
        "            \"decoder_mask\": (target_tokens != self.pad_token).unsqueeze(0).int() & causal_mask(target_tokens.size(0)).clone(),\n",
        "\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "id": "gwgR5MtXwDzG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming you have your Dataset class `TranslationDataset` and DataLoader defined\n",
        "# Example DataLoader for your dataset\n",
        "'''dataset1 = TranslationDataset(level1)  # Your dataframe should be defined\n",
        "train_dataloader_1 = DataLoader(dataset1, batch_size = 32, shuffle=True)  # Set batch_size as needed  # Your dataframe should be defined\n",
        "test_dataloader_1 = DataLoader(dataset1, batch_size=1, shuffle=True)  # Set batch_size as needed\n",
        "\n",
        "dataset2 = TranslationDataset(level2)  # Your dataframe should be defined\n",
        "train_dataloader_2 = DataLoader(dataset2, batch_size = 32, shuffle=True)  # Set batch_size as needed  # Your dataframe should be defined\n",
        "test_dataloader_2 = DataLoader(dataset2, batch_size=1, shuffle=True)  # Set batch_size as needed\n",
        "\n",
        "dataset3 = TranslationDataset(level3)  # Your dataframe should be defined\n",
        "train_dataloader_3 = DataLoader(dataset3, batch_size = 32, shuffle=True)  # Set batch_size as needed  # Your dataframe should be defined\n",
        "test_dataloader_3 = DataLoader(dataset3, batch_size=1, shuffle=True)  # Set batch_size as needed'''\n",
        "\n",
        "dataset4 = TranslationDataset(df)  # Your dataframe should be defined\n",
        "train_dataloader_4 = DataLoader(dataset4, batch_size = 32, shuffle=False)  # Set batch_size as needed  # Your dataframe should be defined\n",
        "# Iterate through batches\n",
        "\n",
        "for batch_idx, batch in enumerate(train_dataloader_4):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "\n",
        "    # Check the shapes of each tensor in the batch\n",
        "    print(f\"  text sequence shape: {batch['source'][0]}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  target text sequence shape: {batch['target_text'][0]}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  source_tokens sequence shape: {batch['source_tokens'].shape}\")  # Expected: (batch_size, T_tamil)\n",
        "    print(f\"  target_tokens token shape: {batch['target_tokens'].shape}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  Combined Tamil mask shape: {batch['decoder_mask'].shape}\")  # Expected: (batch_size, T_tamil_target, T_tamil_target)\n",
        "    break\n",
        "\n",
        "\n",
        "dataset_test = TranslationDataset_test(df)\n",
        "test_dataloader_4 = DataLoader(dataset_test, batch_size=1, shuffle=True)  # Set batch_size as needed\n",
        "# Iterate through batches\n",
        "for batch_idx, batch in enumerate(test_dataloader_4):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "\n",
        "\n",
        "    # Check the shapes of each tensor in the batch\n",
        "    print(f\"  text sequence shape: {torch.tensor(tokenizer.encode(batch['source'][0])).shape}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  target text sequence shape: {batch['target_text'][0]}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  source_tokens sequence shape: {batch['source_tokens'].shape}\")  # Expected: (batch_size, T_tamil)\n",
        "    print(f\"  greedy_tokens token shape: {torch.cat(batch['greedy']).shape}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  target_tokens token shape: {batch['target_tokens'].shape}\")  # Expected: (batch_size, T_english)\n",
        "    print(f\"  Combined Tamil mask shape: {batch['decoder_mask'].shape}\")  # Expected: (batch_size, T_tamil_target, T_tamil_target)\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "id": "U8i5oQzuwDzG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size + len(tokenizer.added_tokens_encoder)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bPgNP7qpwDzH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "tgt_vocab_size = tokenizer.vocab_size + len(tokenizer.added_tokens_encoder)\n",
        "d_model = 64\n",
        "n_heads = 8\n",
        "d_ff = 512\n",
        "n_dec_layers = 8\n",
        "dropout = 0.1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadSelfAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, tgt_mask):\n",
        "        self_attn_output = self.self_attn(x ,x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_heads, d_ff, n_layers, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, tgt, tgt_mask):\n",
        "        x = self.embedding(tgt)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, tgt_mask)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "'''class Transformer(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, d_model, n_heads, d_ff, n_dec_layers, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, n_heads, d_ff, n_dec_layers, dropout)\n",
        "\n",
        "    def forward(self, tgt, tgt_mask):\n",
        "        output = self.decoder(tgt, tgt_mask)\n",
        "        return output'''\n",
        "\n",
        "model = Transformer(tgt_vocab_size, d_model, n_heads, d_ff, n_dec_layers, dropout).to(device)\n",
        "sample_input = torch.randint(0, tgt_vocab_size, (64, 10)).to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "U0CAiLDnwDzH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from num2words import num2words\n",
        "\n",
        "def number_to_words(num):\n",
        "    # Convert number to words in Indian numbering system\n",
        "    return num2words(num, lang='en_IN')\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "# Assuming 'model' is your PyTorch model\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total model parameters: {total_params} -- {number_to_words(total_params)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "URz5Q5jqwDzH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from torch.utils.data import random_split\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "\n",
        "epochs = 15\n",
        "lr = 10**-4\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "def greedy_decode(model, source, max_len, device):\n",
        "    sos_idx, eos_idx = 101, 102\n",
        "    truncate_shape = source.size(-1)\n",
        "    initial_mask = causal_mask(source.size(1)).type_as(source).to(device)\n",
        "    initial_output = model(source, initial_mask)\n",
        "    decoder_input = source.long()\n",
        "    result_tensor = torch.empty((0,), dtype=torch.long).to(device)\n",
        "    while decoder_input.size(1) < max_len:\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source).to(device)\n",
        "        out = model(decoder_input, decoder_mask)\n",
        "        next_word = torch.max(out[:, -1], dim=1)[1]\n",
        "        if next_word == eos_idx: break\n",
        "        result_tensor = torch.cat([result_tensor, next_word.view(1)], dim=0)\n",
        "        decoder_input = torch.cat([decoder_input, next_word.view(1, 1).to(device)], dim=1)\n",
        "\n",
        "\n",
        "    return decoder_input.squeeze(0)[truncate_shape:]\n",
        "    #return result_tensor\n",
        "\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer, max_len, device, print_msg, num_examples=1):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    source_texts, predicted = [], []\n",
        "    try:\n",
        "      console_width = os.get_terminal_size().columns\n",
        "    except OSError:\n",
        "      console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for count, batch in enumerate(validation_ds, start=1):\n",
        "            input = torch.cat(batch[\"greedy\"]).to(device)\n",
        "            input = input.unsqueeze(0).long()\n",
        "            assert input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
        "            model_out = greedy_decode(model, input, max_len, device)\n",
        "\n",
        "            full_text = batch[\"source\"][0]\n",
        "            target_text = batch[\"target_text\"][0]\n",
        "            model_out_text = tokenizer.decode(model_out.detach().cpu().numpy(), skip_special_tokens=True)\n",
        "\n",
        "            source_texts.append(full_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            print_msg(f\"{'FULL TEXT:':>12}{full_text}\\n{'TARGET:':>12}{target_text}\\n{'PREDICTED:':>12}{model_out_text}\\n{'-'*console_width}\")\n",
        "            if count == num_examples: break\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if device == 'cuda':\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "\n",
        "    checkpoint_path = \"GPT_Params.pth\"\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "        print(\"Model loaded from checkpoint.\")\n",
        "    else :\n",
        "        print(\"Checkpoint not found. Training from scratch.\")\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, eps=1e-9)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index= 0).to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "\n",
        "        batch_iterator = tqdm(train_dataloader_4, desc=f\"Processing Epoch {epoch+1:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "            input = batch['source_tokens'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            output = model(input, decoder_mask)\n",
        "\n",
        "            label = batch['target_tokens'].to(device)\n",
        "\n",
        "            loss = loss_fn(output.view(-1, tokenizer.vocab_size + len(tokenizer.added_tokens_encoder)), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        torch.save(model.state_dict(), \"GPT_Params.pth\")\n",
        "        run_validation(model, test_dataloader_4, tokenizer, 300, device, lambda msg: batch_iterator.write(msg))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    train_model()"
      ],
      "metadata": {
        "trusted": true,
        "id": "bhKg6MdRwDzI"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}